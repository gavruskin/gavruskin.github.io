<!doctype html>
<html lang="en">

<head>
	<meta charset="utf-8">
	<title>Introduction to decision trees - Alex Gavryushkin</title>
	<meta name="author" content="Alex Gavryushkin">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
	<meta name="apple-mobile-web-app-capable" content="yes" />
	<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
	<link rel="stylesheet" href="../talks/css/reveal.css">
	<link rel="stylesheet" href="../talks/css/theme/simple_gavruskin.css" id="theme">
	<link rel="stylesheet" href="../talks/lib/css/zenburn.css">
	<!--[if lt IE 9]>
	<script src="../talks/lib/js/html5shiv.js"></script>
	<![endif]-->
</head>

<body>
<div class="reveal">
<div class="slides">

<section data-background-image="../talks/images/ETH_image_covered_completely_non_transparent.svg">
	<br>
	<br>
	<br>
	<br>
	<br>
	<br>
	<br>
	<h4>Lecture 5<br>
	Introduction to classification and
	regression trees</h4>
	<font size="6">Alex Gavryushkin</font><br>
	<font size="5">31 October 2017</font>
</section>

<section>
	<h3>Reminder</h3>
	<ul>
	  <li> All slides are online at <ul><a href="http://alex.gavruskin.com/teaching/">http://alex.gavruskin.com/teaching/</a></ul>
	  <li> You can follow this lecture:<ul><a href="http://alex.gavruskin.com/teaching/STAT203_Lecture5">http://alex.gavruskin.com/teaching/STAT203_Lecture5</a></ul>
	  <li> Lecture notes with embedded runnable code are<font color="red">*</font> at <ul><a href="http://alex.gavruskin.com/teaching/">http://alex.gavruskin.com/teaching/STAT203_Lecture_Notes</a></ul>
	</ul>
	<p style="line-height:0.7">
	<font size="4">
	Some of the figures in this presentation are taken from "An Introduction to Statistical Learning, with applications in R" (Springer, 2013) with permission from the authors: G. James, D. Witten, T. Hastie, and R. Tibshirani.
	</font>
	</p>
	<br>
	<br>
	<br>
	<font color="red" size="3">[*Not quite. The lecture notes will be posted only before the actual lecture.]</font>
</section>

<section>
	<h3>Motivation slide</h3>
	"Many scientists are very capable of analysing their data by themselves." &mdash; Nicolai Meinshausen, Professor of Statistics, ETH Zurich<br>
	<img class="stretch" data-src="images/Meinshausen.jpg"><br>
	<font size="2">Image: ETH Zurich.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.ethz.ch/en/news-and-events/eth-news/news/2017/10/interview-nicolai-meinshausen.html"><u>Complete interview "Our Master's programme is in great demand".</u></a></font><br>
	
	<font color="red" size="3">[I use these to share recent news in the area and to provide further motivation for the students to study the subject.]</font>
</section>

<section>
	<h3>Data <font color="grey">Hitters</font></h3>
	<img data-src="images/data_hitters.png">
	<h3>Problem</h3>
	Can we predict player's salary?<br>
	What drives player's salary the most?
	<br>
	<br>
	<br>
	<font color="red" size="3">[In the actual lecture some other data set will be used, more relevant to the students.]</font>
</section>

<section>
	<img width="60%" data-src="images/hitters_regression_tree.svg">
</section>

<section>
	<img width="50%" data-src="images/hitters_regression_region.svg"><br>
	<font size="5"> Can be seen as fitting a piecewise-constant model.
	The goal is to minimise RSS<br><br>
	$$\sum_{i \in R_1}(y_i - \hat y _{R_1})^2 + \sum_{i \in R_2}(y_i - \hat y _{R_2})^2 + \sum_{i \in R_3}(y_i - \hat y _{R_3})^2$$</font>
</section>

<section>
	<h3>This week's tutorials</h3>
	<ul>
	  <li> Predict NZ house prices
	  <li> Predict cancer patients' life length expectancy
	</ul>
	<br>
	<br>
	<br>
	<br>
	<br>
	<br>
	<br>
	<font color="red" size="3">[Also shows the wide range of relevant applications.]</font>
</section>

<section>
	<img width="30%" data-src="images/hitters_regression_tree.svg">
	<img data-src="images/data_hitters.png">
</section>

<section>
  <section>
	<p align="left">
	  In reality, we don't know what the most significant predictors are, so we need to find them.
	</p>
	<p class="fragment" align="left">
	  We have to consider all predictors and all possible values of the cutpoint for each of the predictors.
	</p>
	<p class="fragment">
	  <img width="20%" data-src="images/computer_mad.svg"><br>
	  ?
	</p>
  </section>
  <section align="left">
	<h3>Details</h3>
	For any $j$ and $s$, we define the pair of half-planes<br><br>
	$$
	R_1(j, s) = \{X \mid X_j < s\}
	$$
	and
	$$
	R_2(j, s) = \{X \mid X_j \ge s\}
	$$<br>
	and we seek the value of $j$ and $s$ that minimise the equation<br><br>
	$$
	\sum_{i:~x_i \in R_1(j, s)}(y_i - \hat y_{R_1})^2 + \sum_{i:~x_i \in R_2(j, s)}(y_i - \hat y_{R_2})^2
	$$
  </section>
</section>

<section data-transition="slide-in fade-out">
	  <img width="75%" data-src="images/hitters_regression_tree_full.svg">
</section>

<section>
	<h3>Dangers?</h3>
	<ul>
	  <li class="fragment" data-fragment-index="1"> Overfitting</li>
	  <li class="fragment" data-fragment-index="2"> Missing out important predictors
	</ul>
	<p class="fragment" data-fragment-index="2">
	  (The bias-variance trade-off)
	</p>
	<img width="60%" data-src="images/hitters_cross_validation_graph.svg">
</section>

<section>
  <section data-transition="fade-in slide-out">
	  <img width="50%" data-src="images/hitters_regression_tree_full.svg"><br>
	  <img width="10%" data-src="images/computer_mad.svg"><br>
	  !
  </section>

  <section>
	<h3>Why not fitting all trees?</h3>
	<ul>
	  <li> The number of full trees, as well as the number of subtrees of a full tree is bounded from below by $2^n$.
	  <li class="fragment" data-fragment-index="1"> $2^n$ is already way too many.
	    Assuming it takes one second to fit a tree:
	</ul>
	<table class="fragment" data-fragment-index="1">
	  <tr>
	    <td>
	      17 minutes
	    </td>
	    <td>
	      on 10 predictors
	    </td>
	  </tr>
	  <tr>
	    <td>
	      12 days
	    </td>
	    <td>
	      on 20 predictors
	    </td>
	  </tr>
	  <tr>
	    <td>
	      34 years
	    </td>
	    <td>
	      on 30 predictors
	    </td>
	  </tr>
	  <tr>
	    <td>
	      35 million years
	    </td>
	    <td>
	      on 50 predictors
	    </td>
	  </tr>
	  <tr>
	    <td>
	      $4 \times 10^{13}$ billion years
	    </td>
	    <td>
	      on 100 predictors
	    </td>
	  </tr>
	</table>
  </section>

  <section>
	<h3>Cost complexity pruning</h3>
	Instead of considering every possible subtree and RSS, minimise
	$$
	\sum_{m = 1}^{|T|}\sum_{i:~x_i \in R_m}(y_i - \hat y_{R_m})^2 + \alpha |T|
	$$
	<br>
	(same idea as in lasso)
	<br>
	<br>
	Importantly, computing subtrees $T$ as a function of $\alpha$ is easy!
  </section>
</section>

<section>
	<h3>Data <font color="grey">Heart</font></h3>
	<img width="65%" data-src="images/data_heart.png">
	<h3>Problem</h3>
	Can we predict whether the patient has heart disease?
</section>

<section data-transition="slide-in fade-out">
	<img class="stretch" data-src="images/heart_trees_validation.svg">
</section>

<section data-transition="fade-in slide-out">
  <section>
	<img class="stretch" data-src="images/heart_trees_validation_highlight.svg">
  </section>

  <section>
	<h3>Error functions</h3>
	<br>
	<table>
	  <tr>
	    <td>Classification error rate</td>
	    <td>$E = 1 - \max\limits_k(\hat p_{mk})$</td>
	  </tr>
	  <tr>
	    <td>Gini index</td>
	    <td>$G = \sum\limits_{k=1}^{K}\hat p_{mk}(1 - \hat p_{mk})$</td>
	  </tr>
	  <tr>
	    <td>Entropy</td>
	    <td>$D = -\sum\limits_{k=1}^K\hat p_{mk}\log\hat p_{mk}$</td>
	  </tr>
	</table>
  </section>
</section>

<section>
	<h3>This week's tutorials</h3>
	<ul>
	  <li> Predict the 2017 general election preferences
	  <li> Predict tumour recurrence in patients treated for melanoma
	</ul>
</section>

<section data-transition="fade-in" data-background-image="../talks/images/ethKuppel.svg">
	<h3>
	Thanks for your attention!
	</h3>
</section>

</div>
</div>

<script src="../talks/lib/js/head.min.js"></script>
<script src="../talks/js/reveal.js"></script>
<script>
  Reveal.initialize({
    dependencies: [
	// Speaker notes
	{ src: '../talks/plugin/notes/notes.js', async: true },
        // Interpret Markdown in <section> elements
        { src: '../talks/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
        { src: '../talks/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
        // MathJax
        { src: '../talks/plugin/math/math.js', async: true }
    ]
  });
  Reveal.configure({ slideNumber: true });
</script>
</body>
</html>
